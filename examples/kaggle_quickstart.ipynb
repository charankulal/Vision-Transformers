{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) - Kaggle Quickstart\n",
    "\n",
    "This notebook demonstrates how to use the Vision Transformer implementation on Kaggle.\n",
    "\n",
    "**Steps:**\n",
    "1. Add this dataset as input to your notebook\n",
    "2. Run the cells below\n",
    "3. Start using ViT in your projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vision Transformer package imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Work in local src directory\n",
    "# Update this path if your dataset has a different name\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import ViT models\n",
    "from vit_all_variants import (\n",
    "    create_vit_s16, create_vit_s32,\n",
    "    create_vit_b16, create_vit_b32,\n",
    "    create_vit_l16, create_vit_l32,\n",
    "    create_vit_h14, create_vit_h16,\n",
    "    list_variants, compare_variants\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vision Transformer package imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the ViT package to Python path\n",
    "# Update this path if your dataset has a different name\n",
    "sys.path.append('/kaggle/input/vision-transformer-all-variants/src')\n",
    "\n",
    "# Import ViT models\n",
    "from vit_all_variants import (\n",
    "    create_vit_s16, create_vit_s32,\n",
    "    create_vit_b16, create_vit_b32,\n",
    "    create_vit_l16, create_vit_l32,\n",
    "    create_vit_h14, create_vit_h16,\n",
    "    list_variants, compare_variants\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vision Transformer package imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: List All Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Available Vision Transformer Variants\n",
      "================================================================================\n",
      "\n",
      "VIT_S16\n",
      "  Patch size: 16x16\n",
      "  Layers: 12\n",
      "  Embed dim: 384\n",
      "  Attention heads: 6\n",
      "  MLP dim: 1536\n",
      "  Parameters: 21,665,664\n",
      "\n",
      "VIT_S32\n",
      "  Patch size: 32x32\n",
      "  Layers: 12\n",
      "  Embed dim: 384\n",
      "  Attention heads: 6\n",
      "  MLP dim: 1536\n",
      "  Parameters: 22,493,952\n",
      "\n",
      "VIT_B16\n",
      "  Patch size: 16x16\n",
      "  Layers: 12\n",
      "  Embed dim: 768\n",
      "  Attention heads: 12\n",
      "  MLP dim: 3072\n",
      "  Parameters: 85,798,656\n",
      "\n",
      "VIT_B32\n",
      "  Patch size: 32x32\n",
      "  Layers: 12\n",
      "  Embed dim: 768\n",
      "  Attention heads: 12\n",
      "  MLP dim: 3072\n",
      "  Parameters: 87,455,232\n",
      "\n",
      "VIT_L16\n",
      "  Patch size: 16x16\n",
      "  Layers: 24\n",
      "  Embed dim: 1024\n",
      "  Attention heads: 16\n",
      "  MLP dim: 4096\n",
      "  Parameters: 303,301,632\n",
      "\n",
      "VIT_L32\n",
      "  Patch size: 32x32\n",
      "  Layers: 24\n",
      "  Embed dim: 1024\n",
      "  Attention heads: 16\n",
      "  MLP dim: 4096\n",
      "  Parameters: 305,510,400\n",
      "\n",
      "VIT_H14\n",
      "  Patch size: 14x14\n",
      "  Layers: 32\n",
      "  Embed dim: 1280\n",
      "  Attention heads: 16\n",
      "  MLP dim: 5120\n",
      "  Parameters: 630,764,800\n",
      "\n",
      "VIT_H16\n",
      "  Patch size: 16x16\n",
      "  Layers: 32\n",
      "  Embed dim: 1280\n",
      "  Attention heads: 16\n",
      "  MLP dim: 5120\n",
      "  Parameters: 630,918,400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all available variants with specifications\n",
    "list_variants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Variant      Patch    Layers   Embed    Heads    Params          Tokens    \n",
      "====================================================================================================\n",
      "VIT_S16      16       12       384      6          21,665,664   197       \n",
      "VIT_S32      32       12       384      6          22,493,952   50        \n",
      "VIT_B16      16       12       768      12         85,798,656   197       \n",
      "VIT_B32      32       12       768      12         87,455,232   50        \n",
      "VIT_L16      16       24       1024     16        303,301,632   197       \n",
      "VIT_L32      32       24       1024     16        305,510,400   50        \n",
      "VIT_H14      14       32       1280     16        630,764,800   257       \n",
      "VIT_H16      16       32       1280     16        630,918,400   197       \n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare all variants side-by-side\n",
    "compare_variants(image_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Feature Extraction\n",
    "\n",
    "Extract features from images for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Total parameters: 85,798,656\n"
     ]
    }
   ],
   "source": [
    "# Create a ViT-B/16 model for feature extraction\n",
    "model = create_vit_b16(\n",
    "    image_size=224,\n",
    "    include_top=False  # No classification head\n",
    ")\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens shape: (4, 197, 768)\n",
      "  -> (batch_size=4, num_tokens=197, embed_dim=768)\n",
      "\n",
      "Class token features shape: (4, 768)\n",
      "  -> (batch_size=4, embed_dim=768)\n",
      "\n",
      "âœ“ These 768-dimensional vectors can be used for:\n",
      "  â€¢ Image similarity search\n",
      "  â€¢ Transfer learning\n",
      "  â€¢ Clustering\n",
      "  â€¢ Downstream classification tasks\n"
     ]
    }
   ],
   "source": [
    "# Create dummy images (replace with your actual images)\n",
    "batch_size = 4\n",
    "images = tf.random.normal((batch_size, 224, 224, 3))\n",
    "\n",
    "# Extract all tokens\n",
    "all_features = model(images, training=False)\n",
    "print(f\"All tokens shape: {all_features.shape}\")\n",
    "print(f\"  -> (batch_size={batch_size}, num_tokens=197, embed_dim=768)\")\n",
    "\n",
    "# Extract class token features only\n",
    "cls_features = model.extract_features(images, training=False)\n",
    "print(f\"\\nClass token features shape: {cls_features.shape}\")\n",
    "print(f\"  -> (batch_size={batch_size}, embed_dim=768)\")\n",
    "print(f\"\\nâœ“ These 768-dimensional vectors can be used for:\")\n",
    "print(\"  â€¢ Image similarity search\")\n",
    "print(\"  â€¢ Transfer learning\")\n",
    "print(\"  â€¢ Clustering\")\n",
    "print(\"  â€¢ Downstream classification tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Image Classification\n",
    "\n",
    "Use ViT with a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier created with 10 output classes\n",
      "Total parameters: 87,462,922\n"
     ]
    }
   ],
   "source": [
    "# Create model with classification head\n",
    "num_classes = 10  # e.g., your dataset classes\n",
    "\n",
    "classifier = create_vit_b32(  # Using B/32 for faster training\n",
    "    image_size=224,\n",
    "    include_top=True,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Classifier created with {num_classes} output classes\")\n",
    "print(f\"Total parameters: {classifier.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model compiled and ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model compiled and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (4, 10)\n",
      "Probabilities shape: (4, 10)\n",
      "\n",
      "Sample prediction for first image:\n",
      "  Predicted class: 1\n",
      "  Confidence: 0.6992\n"
     ]
    }
   ],
   "source": [
    "# Test prediction (with dummy data)\n",
    "test_images = tf.random.normal((4, 224, 224, 3))\n",
    "logits = classifier(test_images, training=False)\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"\\nSample prediction for first image:\")\n",
    "print(f\"  Predicted class: {tf.argmax(probabilities[0]).numpy()}\")\n",
    "print(f\"  Confidence: {tf.reduce_max(probabilities[0]).numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Transfer Learning with Frozen Backbone\n",
    "\n",
    "Use ViT features with custom classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model frozen: trainable=False\n",
      "Base model parameters: 85,798,656\n"
     ]
    }
   ],
   "source": [
    "# Create base model and freeze it\n",
    "base_model = create_vit_b16(image_size=224, include_top=False)\n",
    "base_model.trainable = False  # Freeze the backbone\n",
    "\n",
    "print(f\"Base model frozen: trainable={base_model.trainable}\")\n",
    "print(f\"Base model parameters: {base_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ckula\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Transfer learning model created!\n",
      "  Total parameters: 85,998,090\n",
      "  Trainable parameters: 199,434\n",
      "  Non-trainable parameters: 85,798,656\n",
      "\n",
      "âœ“ Only the custom head will be trained, keeping ViT frozen!\n"
     ]
    }
   ],
   "source": [
    "# Build transfer learning model\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = x[:, 0, :]  # Extract class token\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "transfer_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Show parameter counts\n",
    "total_params = transfer_model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in transfer_model.trainable_weights])\n",
    "non_trainable_params = sum([tf.size(w).numpy() for w in transfer_model.non_trainable_weights])\n",
    "\n",
    "print(f\"\\nTransfer learning model created!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"\\nâœ“ Only the custom head will be trained, keeping ViT frozen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Comparing Different Variants\n",
    "\n",
    "Test different models for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "================================================================================\n",
      "Model        Parameters      Output Shape              Speed     \n",
      "================================================================================\n",
      "ViT-S/32       22,493,952   (1, 50, 384)              âš¡ Fast    \n",
      "ViT-S/16       21,665,664   (1, 197, 384)             âš¡ Fast    \n",
      "ViT-B/32       87,455,232   (1, 50, 768)              â†’ Medium  \n",
      "ViT-B/16       85,798,656   (1, 197, 768)             â†’ Medium  \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create different variants\n",
    "variants = {\n",
    "    'ViT-S/32': create_vit_s32,\n",
    "    'ViT-S/16': create_vit_s16,\n",
    "    'ViT-B/32': create_vit_b32,\n",
    "    'ViT-B/16': create_vit_b16,\n",
    "}\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<12} {'Parameters':<15} {'Output Shape':<25} {'Speed':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_image = tf.random.normal((1, 224, 224, 3))\n",
    "\n",
    "for name, create_fn in variants.items():\n",
    "    model = create_fn(image_size=224, include_top=False)\n",
    "    output = model(test_image, training=False)\n",
    "    params = model.count_params()\n",
    "    \n",
    "    # Simple speed indicator (smaller models are faster)\n",
    "    if params < 30_000_000:\n",
    "        speed = \"âš¡ Fast\"\n",
    "    elif params < 100_000_000:\n",
    "        speed = \"â†’ Medium\"\n",
    "    else:\n",
    "        speed = \"ðŸ¢ Slow\"\n",
    "    \n",
    "    print(f\"{name:<12} {params:>12,}   {str(tuple(output.shape)):<25} {speed:<10}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model weights saved to: vit_s16_weights.weights.h5\n",
      "âœ“ Model weights loaded successfully!\n",
      "\n",
      "Output difference: 0.0000000000 (should be ~0)\n",
      "âœ“ Models are identical!\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = create_vit_s16(image_size=224, include_top=False)\n",
    "\n",
    "# Save weights\n",
    "save_path = \"vit_s16_weights.weights.h5\"\n",
    "model.save_weights(save_path)\n",
    "print(f\"âœ“ Model weights saved to: {save_path}\")\n",
    "\n",
    "# Load weights\n",
    "new_model = create_vit_s16(\n",
    "    image_size=224,\n",
    "    include_top=False,\n",
    "    weights=save_path\n",
    ")\n",
    "print(f\"âœ“ Model weights loaded successfully!\")\n",
    "\n",
    "# Verify they produce same outputs\n",
    "test_image = tf.random.normal((1, 224, 224, 3))\n",
    "original_output = model(test_image, training=False)\n",
    "loaded_output = new_model(test_image, training=False)\n",
    "difference = tf.reduce_max(tf.abs(original_output - loaded_output)).numpy()\n",
    "\n",
    "print(f\"\\nOutput difference: {difference:.10f} (should be ~0)\")\n",
    "print(\"âœ“ Models are identical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now you're ready to use Vision Transformers in your Kaggle projects!\n",
    "\n",
    "**Recommendations:**\n",
    "1. Start with **ViT-B/32** or **ViT-S/16** for quick experimentation\n",
    "2. Use **ViT-B/16** for balanced accuracy and speed\n",
    "3. Try **ViT-L/16** if you have GPU time and want maximum accuracy\n",
    "4. Always use `include_top=False` for feature extraction\n",
    "5. Freeze the backbone initially, then fine-tune if needed\n",
    "\n",
    "**For More Examples:**\n",
    "- Check the `examples/example_usage.py` file in this dataset\n",
    "- Read the README.md for detailed documentation\n",
    "- Experiment with different image sizes and configurations\n",
    "\n",
    "Happy Kaggling! ðŸ†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
